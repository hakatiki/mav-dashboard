{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# MÁV Train Schedule Route Extractor\n",
        "\n",
        "This notebook extracts station pairs (source and destination) from the Hungarian MÁV train schedule PDF.\n",
        "We'll focus on routes like \"Budapest — Hegyeshalom — Rajka\" and extract the first and last stations for each train line.\n",
        "\n",
        "## Features:\n",
        "- Extract station pairs from specific route sections\n",
        "- Handle Hungarian character encoding properly\n",
        "- Export results to CSV for further analysis\n",
        "- Visualize the extracted data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ pdfplumber already installed\n",
            "✓ pandas already installed\n",
            "✓ matplotlib already installed\n",
            "✓ seaborn already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already installed\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Required packages\n",
        "required_packages = [\n",
        "    \"pdfplumber\",\n",
        "    \"pandas\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\"\n",
        "]\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"✓ {package} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        install_package(package)\n",
        "        print(f\"✓ {package} installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import re\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import datetime as dt\n",
        "import csv\n",
        "\n",
        "# Set matplotlib to display Hungarian characters properly\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Segoe UI']\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PDF found: ..\\mav\\2024-2025._evi_belfoldi_kozforgalmu_menetrend_06.21_-_12.13-ig_v06.25.pdf\n",
            "File size: 3.7 MB\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "PDF_PATH = Path(\"../mav/2024-2025._evi_belfoldi_kozforgalmu_menetrend_06.21_-_12.13-ig_v06.25.pdf\")\n",
        "\n",
        "# Check if PDF exists\n",
        "if PDF_PATH.exists():\n",
        "    print(f\"✓ PDF found: {PDF_PATH}\")\n",
        "    print(f\"File size: {PDF_PATH.stat().st_size / (1024*1024):.1f} MB\")\n",
        "else:\n",
        "    print(f\"❌ PDF not found at: {PDF_PATH}\")\n",
        "    print(\"Please check the file path and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 TESTING EXTRACTION ON FIRST 5 PAGES\n",
            "==================================================\n",
            "🔍 Starting extraction from: ..\\mav\\2024-2025._evi_belfoldi_kozforgalmu_menetrend_06.21_-_12.13-ig_v06.25.pdf\n",
            "📄 Processing first 5 pages for testing\n",
            "📖 Total pages in PDF: 724\n",
            "🔄 Will process: 5 pages\n",
            "\n",
            "🔍 Page 2, Found route header: '1 Budapest — Hegyeshalom — Rajka'\n",
            "   ✅ Extracted: Budapest → Rajka (3 stations)\n",
            "\n",
            "🎯 EXTRACTION COMPLETE!\n",
            "   📊 Total routes found: 2\n",
            "   📄 Pages with routes: 2\n",
            "\n",
            "📋 SAMPLE RESULTS:\n",
            "   1. Budapest → Rajka (Page 2)\n",
            "   2. Budapest → Rajka (Page 4)\n"
          ]
        }
      ],
      "source": [
        "def extract_route_pairs_from_pdf(pdf_path, max_pages=10, debug=True):\n",
        "    \"\"\"\n",
        "    Extract station pairs from MÁV PDF by parsing route headers correctly.\n",
        "    \n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        max_pages: Maximum number of pages to process (use None for all pages)\n",
        "        debug: Whether to show debug information\n",
        "    \n",
        "    Returns:\n",
        "        List of dictionaries with route information\n",
        "    \"\"\"\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"🔍 Starting extraction from: {pdf_path}\")\n",
        "        if max_pages:\n",
        "            print(f\"📄 Processing first {max_pages} pages for testing\")\n",
        "        else:\n",
        "            print(f\"📄 Processing ALL pages\")\n",
        "    \n",
        "    all_routes = []\n",
        "    \n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        total_pages = len(pdf.pages)\n",
        "        pages_to_process = pdf.pages[:max_pages] if max_pages else pdf.pages\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"📖 Total pages in PDF: {total_pages}\")\n",
        "            print(f\"🔄 Will process: {len(pages_to_process)} pages\")\n",
        "        \n",
        "        for i, page in enumerate(pages_to_process, 1):\n",
        "            try:\n",
        "                text = page.extract_text()\n",
        "                if not text:\n",
        "                    continue\n",
        "                \n",
        "                lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "                \n",
        "                # Find route headers with em-dash or en-dash\n",
        "                for line in lines:\n",
        "                    if ('—' in line or '–' in line) and re.search(r'\\d+', line):\n",
        "                        \n",
        "                        if debug and i <= 3:  # Show debug for first 3 pages\n",
        "                            print(f\"\\n🔍 Page {i}, Found route header: '{line}'\")\n",
        "                        \n",
        "                        # Parse the route header\n",
        "                        clean_header = line.replace('—', '|').replace('–', '|')\n",
        "                        parts = [part.strip() for part in clean_header.split('|') if part.strip()]\n",
        "                        \n",
        "                        if len(parts) >= 2:\n",
        "                            # Remove leading numbers from first part\n",
        "                            first_part = re.sub(r'^\\d+\\s+', '', parts[0]).strip()\n",
        "                            last_part = parts[-1].strip()\n",
        "                            \n",
        "                            # Extract just the city names (Hungarian characters supported)\n",
        "                            start_match = re.search(r'^([A-Za-záéíóöőúüűÁÉÍÓÖŐÚÜŰ]+(?:\\s+[A-Za-záéíóöőúüűÁÉÍÓÖŐÚÜŰ]+)*)', first_part)\n",
        "                            end_match = re.search(r'^([A-Za-záéíóöőúüűÁÉÍÓÖŐÚÜŰ]+(?:\\s+[A-Za-záéíóöőúüűÁÉÍÓÖŐÚÜŰ]+)*)', last_part)\n",
        "                            \n",
        "                            if start_match and end_match:\n",
        "                                start = start_match.group(1).strip()\n",
        "                                end = end_match.group(1).strip()\n",
        "                                \n",
        "                                # Only keep if both stations are valid and different\n",
        "                                if start and end and start != end and len(start) > 2 and len(end) > 2:\n",
        "                                    route_info = {\n",
        "                                        'page': i,\n",
        "                                        'route_header': line,\n",
        "                                        'source': start,\n",
        "                                        'destination': end,\n",
        "                                        'total_stations': len(parts),\n",
        "                                        'intermediate_stations': parts[1:-1] if len(parts) > 2 else []\n",
        "                                    }\n",
        "                                    all_routes.append(route_info)\n",
        "                                    \n",
        "                                    if debug and i <= 3:\n",
        "                                        print(f\"   ✅ Extracted: {start} → {end} ({len(parts)} stations)\")\n",
        "            \n",
        "            except Exception as e:\n",
        "                if debug:\n",
        "                    print(f\"   ⚠️  Error on page {i}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"\\n🎯 EXTRACTION COMPLETE!\")\n",
        "        print(f\"   📊 Total routes found: {len(all_routes)}\")\n",
        "        print(f\"   📄 Pages with routes: {len(set(r['page'] for r in all_routes))}\")\n",
        "        \n",
        "        if all_routes:\n",
        "            print(f\"\\n📋 SAMPLE RESULTS:\")\n",
        "            for i, route in enumerate(all_routes[:5], 1):\n",
        "                print(f\"   {i}. {route['source']} → {route['destination']} (Page {route['page']})\")\n",
        "    \n",
        "    return all_routes\n",
        "\n",
        "# Test extraction on first 5 pages\n",
        "print(\"🧪 TESTING EXTRACTION ON FIRST 5 PAGES\")\n",
        "print(\"=\"*50)\n",
        "test_routes = extract_route_pairs_from_pdf(PDF_PATH, max_pages=5, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 RUNNING FULL EXTRACTION ON ALL 724 PAGES\n",
            "==================================================\n",
            "⏳ This may take 5-10 minutes...\n",
            "\n",
            "✅ EXTRACTION COMPLETED!\n",
            "   ⏱️  Time: 59.8 seconds\n",
            "   📊 Routes found: 398\n",
            "   📄 Pages with data: 352\n",
            "\n",
            "📋 SAMPLE RESULTS:\n",
            "    1. Budapest → Rajka (Page 2)\n",
            "    2. Budapest → Rajka (Page 4)\n",
            "    3. Budapest → Rajka (Page 6)\n",
            "    4. Budapest → Rajka (Page 8)\n",
            "    5. Budapest → Rajka (Page 10)\n",
            "    6. Budapest → Rajka (Page 12)\n",
            "    7. Budapest → Rajka (Page 14)\n",
            "    8. Budapest → Rajka (Page 16)\n",
            "    9. Budapest → Rajka (Page 18)\n",
            "   10. Budapest → Rajka (Page 20)\n"
          ]
        }
      ],
      "source": [
        "# Run extraction on ALL pages\n",
        "print(\"🚀 RUNNING FULL EXTRACTION ON ALL 724 PAGES\")\n",
        "print(\"=\"*50)\n",
        "print(\"⏳ This may take 5-10 minutes...\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Extract from all pages\n",
        "all_routes = extract_route_pairs_from_pdf(PDF_PATH, max_pages=None, debug=False)\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "print(f\"\\n✅ EXTRACTION COMPLETED!\")\n",
        "print(f\"   ⏱️  Time: {processing_time:.1f} seconds\")\n",
        "print(f\"   📊 Routes found: {len(all_routes)}\")\n",
        "print(f\"   📄 Pages with data: {len(set(r['page'] for r in all_routes))}\")\n",
        "\n",
        "# Show sample results\n",
        "print(f\"\\n📋 SAMPLE RESULTS:\")\n",
        "for i, route in enumerate(all_routes[:10], 1):\n",
        "    print(f\"   {i:2d}. {route['source']} → {route['destination']} (Page {route['page']})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 SAVING RESULTS TO CSV\n",
            "==============================\n",
            "✅ All routes: mav_all_routes_20250715_221609.csv (398 routes)\n",
            "✅ Unique pairs: mav_unique_pairs_20250715_221609.csv (132 pairs)\n",
            "✅ Budapest-Rajka: budapest_rajka_routes_20250715_221609.csv (24 routes)\n",
            "\n",
            "🎯 BUDAPEST-RAJKA PAIRS FOUND:\n",
            "   1. Budapest → Rajka\n",
            "   2. Rajka → Budapest\n",
            "\n",
            "🎉 SUCCESS! Files saved with timestamp: 20250715_221609\n"
          ]
        }
      ],
      "source": [
        "# Save results to CSV files\n",
        "print(\"\\n💾 SAVING RESULTS TO CSV\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "if all_routes:\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_routes)\n",
        "    \n",
        "    # Create timestamp for file naming\n",
        "    timestamp = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # 1. Save all routes\n",
        "    filename_all = f\"mav_all_routes_{timestamp}.csv\"\n",
        "    df.to_csv(filename_all, index=False, encoding='utf-8')\n",
        "    print(f\"✅ All routes: {filename_all} ({len(df)} routes)\")\n",
        "    \n",
        "    # 2. Save unique pairs only\n",
        "    df_unique = df.drop_duplicates(['source', 'destination'])[['source', 'destination', 'total_stations']]\n",
        "    filename_unique = f\"mav_unique_pairs_{timestamp}.csv\"\n",
        "    df_unique.to_csv(filename_unique, index=False, encoding='utf-8')\n",
        "    print(f\"✅ Unique pairs: {filename_unique} ({len(df_unique)} pairs)\")\n",
        "    \n",
        "    # 3. Save Budapest-Rajka routes specifically\n",
        "    budapest_routes = df[\n",
        "        (df['route_header'].str.contains('Budapest', case=False)) & \n",
        "        (df['route_header'].str.contains('Rajka|Hegyeshalom', case=False))\n",
        "    ]\n",
        "    \n",
        "    if len(budapest_routes) > 0:\n",
        "        filename_budapest = f\"budapest_rajka_routes_{timestamp}.csv\"\n",
        "        budapest_routes.to_csv(filename_budapest, index=False, encoding='utf-8')\n",
        "        print(f\"✅ Budapest-Rajka: {filename_budapest} ({len(budapest_routes)} routes)\")\n",
        "        \n",
        "        print(f\"\\n🎯 BUDAPEST-RAJKA PAIRS FOUND:\")\n",
        "        unique_budapest = budapest_routes.drop_duplicates(['source', 'destination'])\n",
        "        for i, (_, row) in enumerate(unique_budapest.iterrows(), 1):\n",
        "            print(f\"   {i}. {row['source']} → {row['destination']}\")\n",
        "    \n",
        "    print(f\"\\n🎉 SUCCESS! Files saved with timestamp: {timestamp}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No routes to save\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
